{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark and its config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set number of partitions each rdd be to 8, because of number of cores for parallelism of SON algorithm. so we have 8 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import *\n",
    "\n",
    "# create conf\n",
    "conf = SparkConf().setAppName(\"Frequent Itemset\")\n",
    "conf.set(\"spark.default.parallelism\", 8)\n",
    "conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using spark function to load data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>103288</td>\n",
       "      <td>i was plagiarized by arbitrage magazine but i ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>50219</td>\n",
       "      <td>i feel resigned to what happens to me</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>65485</td>\n",
       "      <td>i guess because i feel emotionally i have been...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>101735</td>\n",
       "      <td>i only want people in my life who make me feel...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>122148</td>\n",
       "      <td>i was still feeling tooheys tuesdays in my leg...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text emotions\n",
       "0        27383  i feel awful about it too because it s my job ...  sadness\n",
       "1       110083                              im alone i feel awful  sadness\n",
       "2       140764  ive probably mentioned this before but i reall...      joy\n",
       "3       100071           i was feeling a little low few days back  sadness\n",
       "4         2837  i beleive that i am much more sensitive to oth...     love\n",
       "...        ...                                                ...      ...\n",
       "299995  103288  i was plagiarized by arbitrage magazine but i ...  sadness\n",
       "299996   50219              i feel resigned to what happens to me  sadness\n",
       "299997   65485  i guess because i feel emotionally i have been...  sadness\n",
       "299998  101735  i only want people in my life who make me feel...      joy\n",
       "299999  122148  i was still feeling tooheys tuesdays in my leg...      joy\n",
       "\n",
       "[300000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"train.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"True\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "df = df.drop(\"_c0\")\n",
    "display(df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Package for Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "169be6e9-bfe8-4bc7-be0f-e8b670a4bba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 812 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp38-cp38-manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[K     |████████████████████████████████| 738 kB 678 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.50.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=89ee2ae67f121e72d58a151a85654002c1fb20c07110f23cc9371f0fda4f748d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.11.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading libraries of word processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "976f3e0c-2da3-4954-a0e9-e2073b5e311a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing all the tasks defined in question sequentially. they are obvious with comments. After that we have a clean text and we add it to first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding clean text to df :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[feel, awful, job, get, position, succeed, hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[alone, feel, awful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[ive, probably, mention, really, feel, proud, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[feel, little, low, day, back]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "      <td>[beleive, much, sensitive, people, feeling, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>103288</td>\n",
       "      <td>i was plagiarized by arbitrage magazine but i ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[plagiarize, arbitrage, magazine, one, feel, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>50219</td>\n",
       "      <td>i feel resigned to what happens to me</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[feel, resign, happens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>65485</td>\n",
       "      <td>i guess because i feel emotionally i have been...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[guess, feel, emotionally, beaten, circumstance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>101735</td>\n",
       "      <td>i only want people in my life who make me feel...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[want, people, life, make, feel, value, need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>122148</td>\n",
       "      <td>i was still feeling tooheys tuesdays in my leg...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[still, feel, tooheys, tuesday, legs, wasnt, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text emotions  \\\n",
       "0        27383  i feel awful about it too because it s my job ...  sadness   \n",
       "1       110083                              im alone i feel awful  sadness   \n",
       "2       140764  ive probably mentioned this before but i reall...      joy   \n",
       "3       100071           i was feeling a little low few days back  sadness   \n",
       "4         2837  i beleive that i am much more sensitive to oth...     love   \n",
       "...        ...                                                ...      ...   \n",
       "299995  103288  i was plagiarized by arbitrage magazine but i ...  sadness   \n",
       "299996   50219              i feel resigned to what happens to me  sadness   \n",
       "299997   65485  i guess because i feel emotionally i have been...  sadness   \n",
       "299998  101735  i only want people in my life who make me feel...      joy   \n",
       "299999  122148  i was still feeling tooheys tuesdays in my leg...      joy   \n",
       "\n",
       "                                               clean_text  \n",
       "0       [feel, awful, job, get, position, succeed, hap...  \n",
       "1                                    [alone, feel, awful]  \n",
       "2       [ive, probably, mention, really, feel, proud, ...  \n",
       "3                          [feel, little, low, day, back]  \n",
       "4       [beleive, much, sensitive, people, feeling, te...  \n",
       "...                                                   ...  \n",
       "299995  [plagiarize, arbitrage, magazine, one, feel, t...  \n",
       "299996                            [feel, resign, happens]  \n",
       "299997   [guess, feel, emotionally, beaten, circumstance]  \n",
       "299998      [want, people, life, make, feel, value, need]  \n",
       "299999  [still, feel, tooheys, tuesday, legs, wasnt, s...  \n",
       "\n",
       "[300000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    #lower\n",
    "    lower_text = text.lower()\n",
    "    \n",
    "    #delete number\n",
    "    without_num_text = re.sub(r'\\d+', '', lower_text)\n",
    "    \n",
    "    #delete punctutation\n",
    "    without_punc_text = without_num_text.translate((None, string.punctuation))\n",
    "    \n",
    "    #delete stopwords\n",
    "    word_tokens = word_tokenize(without_punc_text)\n",
    "    without_stop_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    #delete words with length less than 3\n",
    "    without_length_text = [word for word in without_stop_text if len(word) > 2]\n",
    "    \n",
    "    #stemming\n",
    "    #stemmer= PorterStemmer()\n",
    "    #stemmed_text = [stemmer.stem(word) for word in without_length_text]\n",
    "    \n",
    "    #lemmatization\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatize_text = []\n",
    "    for token, tag in pos_tag(without_length_text):\n",
    "        lemmatize_text.append(lemmatizer.lemmatize(token, tag_dict[tag[0]]))\n",
    "        \n",
    "    return lemmatize_text\n",
    "\n",
    "# defining spark function for preprocess the data\n",
    "preprocess_udf = udf(lambda x: preprocess(x), ArrayType(StringType()))\n",
    "\n",
    "# list of english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tags needed for lemmatization\n",
    "tag_dict = defaultdict(lambda : wordnet.NOUN)\n",
    "tag_dict['J'] = wordnet.ADJ\n",
    "tag_dict['V'] = wordnet.VERB\n",
    "tag_dict['R'] = wordnet.ADV\n",
    "\n",
    "# cleaning the text\n",
    "df = df.withColumn(\"clean_text\", preprocess_udf(col(\"text\")))\n",
    "\n",
    "print(\"Adding clean text to df :\")\n",
    "display(df.toPandas())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B) Saving distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86ef65c5-fa1f-480b-98e3-172403721d00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# in map phase we iterate on each sentence and for each word we create (word, 1)\n",
    "word_rdd = df.select(col(\"clean_text\")).rdd.flatMap(lambda list_word: [(word, 1) for word in list_word[0]])\n",
    "\n",
    "# in reduce phase we just count distinct words and then we another map just return the words\n",
    "distinct_words = word_rdd.reduceByKey(lambda x,y: x+y).map(lambda word: word[0]).collect()\n",
    "\n",
    "# as the question wanted, we attach unique id to each distinct word\n",
    "distinct_dict = dict(zip(list(range(len(distinct_words))), distinct_words))\n",
    "\n",
    "\n",
    "# save distinct words\n",
    "with open('distinct_words.json', 'w') as fp:\n",
    "    json.dump(distinct_dict, fp)\n",
    "#clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part J) SON algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 8 chunks. Here we show number of sentences per partition(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6203724a-5666-4166-afc7-ce74d3e48d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences per partitions(chunks) : \n",
      "[42372, 42381, 41974, 41858, 41860, 41785, 41773, 5997]\n"
     ]
    }
   ],
   "source": [
    "# creating rdd of sentences and emotions for SON algorithm\n",
    "sentence_rdd = df.select(col(\"clean_text\"), col(\"emotions\")).rdd.map(lambda row: (row['clean_text'], row['emotions']))\n",
    "\n",
    "# number of sentence per partition\n",
    "def countp(part):\n",
    "    baskets = []\n",
    "    for v in part:\n",
    "        baskets.append(v)\n",
    "    return [len(baskets)]\n",
    "\n",
    "# defining SON support and length of all sentence\n",
    "supp = 0.05\n",
    "size_of_sentences = df.count()\n",
    "\n",
    "# counting number of sentence per partition so we can select a accurate support per partition\n",
    "num_per_partition = sentence_rdd.mapPartitions(countp).collect()\n",
    "\n",
    "print(\"Number of sentences per partitions(chunks) : \")\n",
    "print(num_per_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 of SON\n",
    "Here we do first phase of SON algorithm to find candidates for second phase. Our Hypothesis here is, if each frequent item candidate is frequent on more than 2 chunks, is frequent. Other details are in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7d00300a-3914-400d-aa7d-1317e84e083b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some candidates of Phase 1: \n",
      "['feel', 'really', 'like', 'start', 'thought', 'good', 'two', 'book', 'yet', 'work']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# creating subsets of length 2\n",
    "def subset(setInp):\n",
    "    subsets = []\n",
    "    subsets.extend(list(combinations(setInp, 2)))\n",
    "    return subsets\n",
    " \n",
    "# find single frequent items in phase 1 of SON\n",
    "def singleton(sentences, support):\n",
    "    # holding counting of words\n",
    "    bag_of_words = dict()\n",
    "    \n",
    "    # iterating on each sentence\n",
    "    for sentence in sentences:\n",
    "        # iterating on word of sentence\n",
    "        for word in sentence[0]:\n",
    "            \n",
    "            # adding to count of word\n",
    "            if(word in bag_of_words):\n",
    "                bag_of_words[word] += 1\n",
    "        \n",
    "            else:\n",
    "                bag_of_words[word] = 1\n",
    "      \n",
    "     \n",
    "    L1 = []\n",
    "    len_sentence = len(sentences)\n",
    "    \n",
    "    # filter base on support that was calculated base on number of sentences per partition\n",
    "    for word in bag_of_words:\n",
    "        if( bag_of_words[word] / len_sentence >= support):\n",
    "              L1.append((word, 1))\n",
    "  \n",
    "    return L1\n",
    "\n",
    "# find frequent items with length 2 in phase 1\n",
    "def doubleton(sentences, support, candidates):\n",
    "    # holding counting of doubles\n",
    "    bag_of_double = dict()\n",
    "    \n",
    "    # iterating on each sentence\n",
    "    for sentence in sentences:\n",
    "        # iterating on candidates\n",
    "        for candidate in candidates:\n",
    "            \n",
    "            # adding to count of word\n",
    "            if(set(candidate).issubset(sentence[0])):\n",
    "                \n",
    "                if(candidate in bag_of_double):\n",
    "                    bag_of_double[candidate] +=1\n",
    "                else:\n",
    "                    bag_of_double[candidate] = 1\n",
    "        \n",
    "    L2 = []\n",
    "    len_sentence = len(sentences)\n",
    "    # filter base on support that was calculated base on number of sentences per partition\n",
    "    for double in bag_of_double:\n",
    "        if( bag_of_double[double] / len_sentence >= support):\n",
    "            L2.append((double, 1))\n",
    "      \n",
    "    return L2\n",
    "\n",
    "# map function of phase 1 to find candidates\n",
    "def freq_candidate(partition_iterator):\n",
    "    \n",
    "    # first retrieve sentences from partition\n",
    "    sentences = []\n",
    "    for sentence in partition_iterator:\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    # computing support for each partitions base on fraction of their size to all sentences\n",
    "    fraction_of_all = len(sentences) / size_of_sentences\n",
    "    support_per_partition = fraction_of_all * supp \n",
    "    \n",
    "    # computing single frequent itemsets\n",
    "    single_freq = singleton(sentences, support_per_partition)\n",
    "    \n",
    "    # building candaites of doubleton\n",
    "    freq_list = [word[0] for word in single_freq]\n",
    "    candidate_double = subset(freq_list)\n",
    "  \n",
    "    # finding frequent doubleton\n",
    "    double_freq = doubleton(sentences, support_per_partition, candidate_double)\n",
    "  \n",
    "    return single_freq + double_freq\n",
    "\n",
    "# SON algorithm (map 1)\n",
    "son_map1_rdd = sentence_rdd.mapPartitions(freq_candidate)\n",
    "\n",
    "# SON algorithm (reduce 1)\n",
    "son_reduce1_rdd = son_map1_rdd.reduceByKey(lambda x,y: x+y).filter(lambda itemset: itemset[1] > 2).map(lambda itemset: itemset[0])\n",
    "phase1_candidate = son_reduce1_rdd.collect()\n",
    "\n",
    "print(\"Show some candidates of Phase 1: \")\n",
    "print(phase1_candidate[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Phase of SON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e656d4e5-d93f-4c48-b21d-3a8f1796e83b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing two frequent items :\n",
      "[('feel', ({'sadness': 77117, 'love': 22478, 'joy': 92517, 'fear': 29820, 'surprise': 9466, 'anger': 35445}, 266843)), ('really', ({'sadness': 5012, 'love': 1491, 'joy': 5738, 'fear': 1870, 'surprise': 661, 'anger': 2475}, 17247))]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "  \n",
    "def count_cand(sentences):\n",
    "    # dict holding count of each word\n",
    "    bag_of_words = dict()\n",
    "    \n",
    "    # list of available emotions\n",
    "    emotions = ['sadness', 'love','joy', 'fear', 'surprise', 'anger']\n",
    "    \n",
    "    # iterate on candidates of phase 1\n",
    "    for word in phase1_candidate:\n",
    "        \n",
    "        # iterate on sentence\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            # check if it is single or doubleton\n",
    "            if(type(word) is str):\n",
    "                \n",
    "                # check for occurence of word in sentence\n",
    "                if(word in sentence[0]):\n",
    "                    if(word in bag_of_words):\n",
    "                        bag_of_words[word]['Count'] +=1\n",
    "                        bag_of_words[word]['emotion'][sentence[1]] +=1\n",
    "\n",
    "                    else:\n",
    "                        detail = dict()\n",
    "                        detail['Count'] = 1\n",
    "                        detail['emotion'] = dict()\n",
    "                        for i in emotions:\n",
    "                            detail['emotion'][i] = 0 \n",
    "\n",
    "                        detail['emotion'][sentence[1]] = 1\n",
    "                        bag_of_words[word] = detail\n",
    "            \n",
    "            # for doubletons\n",
    "            else:\n",
    "                if( (word[0] in sentence[0]) and (word[1] in sentence[0]) ):\n",
    "                    if(word in bag_of_words):\n",
    "                        bag_of_words[word]['Count'] +=1\n",
    "                        bag_of_words[word]['emotion'][sentence[1]] +=1\n",
    "\n",
    "                    else:\n",
    "                        detail = dict()\n",
    "                        detail['Count'] = 1\n",
    "                        detail['emotion'] = dict()\n",
    "                        for i in emotions:\n",
    "                            detail['emotion'][i] = 0 \n",
    "\n",
    "                        detail['emotion'][sentence[1]] = 1\n",
    "                        bag_of_words[word] = detail\n",
    "\n",
    "    out_tuple = []\n",
    "    # prepare data for reduce\n",
    "    for word in bag_of_words:\n",
    "        out_tuple.append((word, (bag_of_words[word]['emotion'], bag_of_words[word]['Count'])))\n",
    "  \n",
    "    return out_tuple\n",
    "\n",
    "\n",
    "\n",
    "# check candidates of phase 1 with one more iterate on all data\n",
    "def freq_itemset(partition_iterator):\n",
    "    \n",
    "    # retireve sentences from partition\n",
    "    sentences = []\n",
    "    for sentence in partition_iterator:\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    # counting candidates on all sentence and on each emotion\n",
    "    count_cand_list = count_cand(sentences)\n",
    "  \n",
    "    return count_cand_list\n",
    "\n",
    "# SON algorithm (map 2)\n",
    "son_map2_rdd = sentence_rdd.mapPartitions(freq_itemset)\n",
    "\n",
    "# SON algorithm (reduce 2)\n",
    "son_reduce2_rdd = son_map2_rdd.reduceByKey(lambda x,y:(dict(Counter(x[0]) + Counter(y[0])), x[1]+y[1])).filter(lambda itemset: itemset[1][1] / size_of_sentences >= supp )\n",
    "final_freq_list = son_reduce2_rdd.collect()\n",
    "\n",
    "print(\"Showing two frequent items :\")\n",
    "print(final_freq_list[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Freq items as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f606c0d2-e495-4967-879f-a0b0d0244330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing Frequent items :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequent item</th>\n",
       "      <th>sadness</th>\n",
       "      <th>love</th>\n",
       "      <th>joy</th>\n",
       "      <th>fear</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anger</th>\n",
       "      <th>count</th>\n",
       "      <th>max_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel</td>\n",
       "      <td>77117</td>\n",
       "      <td>22478</td>\n",
       "      <td>92517</td>\n",
       "      <td>29820</td>\n",
       "      <td>9466</td>\n",
       "      <td>35445</td>\n",
       "      <td>266843</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>really</td>\n",
       "      <td>5012</td>\n",
       "      <td>1491</td>\n",
       "      <td>5738</td>\n",
       "      <td>1870</td>\n",
       "      <td>661</td>\n",
       "      <td>2475</td>\n",
       "      <td>17247</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>14859</td>\n",
       "      <td>5621</td>\n",
       "      <td>16950</td>\n",
       "      <td>4254</td>\n",
       "      <td>1609</td>\n",
       "      <td>6985</td>\n",
       "      <td>50278</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('feel', 'make')</td>\n",
       "      <td>5412</td>\n",
       "      <td>1534</td>\n",
       "      <td>7330</td>\n",
       "      <td>1983</td>\n",
       "      <td>536</td>\n",
       "      <td>2019</td>\n",
       "      <td>18814</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('feel', 'know')</td>\n",
       "      <td>5038</td>\n",
       "      <td>1459</td>\n",
       "      <td>5195</td>\n",
       "      <td>2130</td>\n",
       "      <td>639</td>\n",
       "      <td>2245</td>\n",
       "      <td>16706</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('feel', 'really')</td>\n",
       "      <td>4539</td>\n",
       "      <td>1358</td>\n",
       "      <td>5307</td>\n",
       "      <td>1691</td>\n",
       "      <td>599</td>\n",
       "      <td>2195</td>\n",
       "      <td>15689</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('feel', 'like')</td>\n",
       "      <td>14070</td>\n",
       "      <td>5227</td>\n",
       "      <td>16290</td>\n",
       "      <td>4013</td>\n",
       "      <td>1538</td>\n",
       "      <td>6588</td>\n",
       "      <td>47726</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>get</td>\n",
       "      <td>6011</td>\n",
       "      <td>1611</td>\n",
       "      <td>7492</td>\n",
       "      <td>2491</td>\n",
       "      <td>765</td>\n",
       "      <td>3752</td>\n",
       "      <td>22122</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('feel', 'get')</td>\n",
       "      <td>5061</td>\n",
       "      <td>1335</td>\n",
       "      <td>6378</td>\n",
       "      <td>2072</td>\n",
       "      <td>617</td>\n",
       "      <td>3134</td>\n",
       "      <td>18597</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>think</td>\n",
       "      <td>4286</td>\n",
       "      <td>1324</td>\n",
       "      <td>4690</td>\n",
       "      <td>1946</td>\n",
       "      <td>625</td>\n",
       "      <td>2204</td>\n",
       "      <td>15075</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feeling</td>\n",
       "      <td>10991</td>\n",
       "      <td>2626</td>\n",
       "      <td>9290</td>\n",
       "      <td>4344</td>\n",
       "      <td>1436</td>\n",
       "      <td>4924</td>\n",
       "      <td>33611</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>time</td>\n",
       "      <td>5120</td>\n",
       "      <td>1373</td>\n",
       "      <td>5624</td>\n",
       "      <td>2017</td>\n",
       "      <td>685</td>\n",
       "      <td>2424</td>\n",
       "      <td>17243</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('feel', 'time')</td>\n",
       "      <td>4566</td>\n",
       "      <td>1235</td>\n",
       "      <td>5167</td>\n",
       "      <td>1766</td>\n",
       "      <td>612</td>\n",
       "      <td>2118</td>\n",
       "      <td>15464</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>make</td>\n",
       "      <td>5746</td>\n",
       "      <td>1613</td>\n",
       "      <td>7738</td>\n",
       "      <td>2150</td>\n",
       "      <td>572</td>\n",
       "      <td>2258</td>\n",
       "      <td>20077</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>know</td>\n",
       "      <td>5628</td>\n",
       "      <td>1610</td>\n",
       "      <td>5678</td>\n",
       "      <td>2377</td>\n",
       "      <td>720</td>\n",
       "      <td>2575</td>\n",
       "      <td>18588</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         frequent item  sadness   love    joy   fear  surprise  anger   count  \\\n",
       "0                 feel    77117  22478  92517  29820      9466  35445  266843   \n",
       "1               really     5012   1491   5738   1870       661   2475   17247   \n",
       "2                 like    14859   5621  16950   4254      1609   6985   50278   \n",
       "3     ('feel', 'make')     5412   1534   7330   1983       536   2019   18814   \n",
       "4     ('feel', 'know')     5038   1459   5195   2130       639   2245   16706   \n",
       "5   ('feel', 'really')     4539   1358   5307   1691       599   2195   15689   \n",
       "6     ('feel', 'like')    14070   5227  16290   4013      1538   6588   47726   \n",
       "7                  get     6011   1611   7492   2491       765   3752   22122   \n",
       "8      ('feel', 'get')     5061   1335   6378   2072       617   3134   18597   \n",
       "9                think     4286   1324   4690   1946       625   2204   15075   \n",
       "10             feeling    10991   2626   9290   4344      1436   4924   33611   \n",
       "11                time     5120   1373   5624   2017       685   2424   17243   \n",
       "12    ('feel', 'time')     4566   1235   5167   1766       612   2118   15464   \n",
       "13                make     5746   1613   7738   2150       572   2258   20077   \n",
       "14                know     5628   1610   5678   2377       720   2575   18588   \n",
       "\n",
       "   max_emotion  \n",
       "0          joy  \n",
       "1          joy  \n",
       "2          joy  \n",
       "3          joy  \n",
       "4          joy  \n",
       "5          joy  \n",
       "6          joy  \n",
       "7          joy  \n",
       "8          joy  \n",
       "9          joy  \n",
       "10     sadness  \n",
       "11         joy  \n",
       "12         joy  \n",
       "13         joy  \n",
       "14         joy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from builtins import max\n",
    "\n",
    "# creating schema of dataframe\n",
    "mySchema = StructType([ StructField(\"frequent item\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"sadness\", IntegerType(), True)\\\n",
    "\n",
    "                       ,StructField(\"love\", IntegerType(), True)\\\n",
    "\n",
    "                       ,StructField(\"joy\", IntegerType(), True)\\\n",
    "\n",
    "                       ,StructField(\"fear\", IntegerType(), True)\\\n",
    "\n",
    "                       ,StructField(\"surprise\", IntegerType(), True)\\\n",
    "\n",
    "                       ,StructField(\"anger\", IntegerType(), True)\\\n",
    "                       ,StructField(\"count\", IntegerType(), True)\\\n",
    "                       ,StructField(\"max_emotion\", StringType(), True)\n",
    "\n",
    "                       ])\n",
    "\n",
    "df_list = []\n",
    "for item in final_freq_list:\n",
    "    \n",
    "    dict_inp = dict()\n",
    "    dict_inp['frequent item'] = str(item[0])\n",
    "    dict_inp.update(item[1][0])\n",
    "    dict_inp['count'] = item[1][1]\n",
    "    dict_inp['max_emotion'] = max(item[1][0], key=item[1][0].get) \n",
    "    df_list.append(dict_inp)\n",
    "   \n",
    "frequent_items_df = spark.createDataFrame(df_list, schema = mySchema).toPandas()\n",
    "print(\"Showing Frequent items :\")\n",
    "display(frequent_items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb3c5ebe-9890-4b67-ab29-10589cb56f5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Part D) Guessing Emotions of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence we iterate on frequent itemsets. every itemsets that exist in sentence vote about its new emotion. Maximum number of votes will be consider as new emotion. But if there is no frequent itemset in sentence, we don't make any guess and just writing '-' at emotion column of that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with new emotion label :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>new_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[feel, awful, job, get, position, succeed, hap...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[alone, feel, awful]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[ive, probably, mention, really, feel, proud, ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[feel, little, low, day, back]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "      <td>[beleive, much, sensitive, people, feeling, te...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>103288</td>\n",
       "      <td>i was plagiarized by arbitrage magazine but i ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[plagiarize, arbitrage, magazine, one, feel, t...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>50219</td>\n",
       "      <td>i feel resigned to what happens to me</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[feel, resign, happens]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>65485</td>\n",
       "      <td>i guess because i feel emotionally i have been...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[guess, feel, emotionally, beaten, circumstance]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>101735</td>\n",
       "      <td>i only want people in my life who make me feel...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[want, people, life, make, feel, value, need]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>122148</td>\n",
       "      <td>i was still feeling tooheys tuesdays in my leg...</td>\n",
       "      <td>joy</td>\n",
       "      <td>[still, feel, tooheys, tuesday, legs, wasnt, s...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text emotions  \\\n",
       "0        27383  i feel awful about it too because it s my job ...  sadness   \n",
       "1       110083                              im alone i feel awful  sadness   \n",
       "2       140764  ive probably mentioned this before but i reall...      joy   \n",
       "3       100071           i was feeling a little low few days back  sadness   \n",
       "4         2837  i beleive that i am much more sensitive to oth...     love   \n",
       "...        ...                                                ...      ...   \n",
       "299995  103288  i was plagiarized by arbitrage magazine but i ...  sadness   \n",
       "299996   50219              i feel resigned to what happens to me  sadness   \n",
       "299997   65485  i guess because i feel emotionally i have been...  sadness   \n",
       "299998  101735  i only want people in my life who make me feel...      joy   \n",
       "299999  122148  i was still feeling tooheys tuesdays in my leg...      joy   \n",
       "\n",
       "                                               clean_text new_emotion  \n",
       "0       [feel, awful, job, get, position, succeed, hap...         joy  \n",
       "1                                    [alone, feel, awful]         joy  \n",
       "2       [ive, probably, mention, really, feel, proud, ...         joy  \n",
       "3                          [feel, little, low, day, back]         joy  \n",
       "4       [beleive, much, sensitive, people, feeling, te...     sadness  \n",
       "...                                                   ...         ...  \n",
       "299995  [plagiarize, arbitrage, magazine, one, feel, t...         joy  \n",
       "299996                            [feel, resign, happens]         joy  \n",
       "299997   [guess, feel, emotionally, beaten, circumstance]         joy  \n",
       "299998      [want, people, life, make, feel, value, need]         joy  \n",
       "299999  [still, feel, tooheys, tuesday, legs, wasnt, s...         joy  \n",
       "\n",
       "[300000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def attach_emotion(sentence):\n",
    "    # holding value of each emotion candidate\n",
    "    emotion_holder = dict.fromkeys(['sadness','love','joy', 'fear', 'surprise', 'anger'],0)\n",
    "    \n",
    "    # iterate on frequent items\n",
    "    for item, emotion in zip(frequent_items_df['frequent item'], frequent_items_df['max_emotion']):\n",
    "        \n",
    "        # condition for single freq\n",
    "        if(type(item) is str):\n",
    "            if(item in sentence):\n",
    "                emotion_holder[emotion] +=1\n",
    "                \n",
    "        else:\n",
    "            if(item[0] in sentence and item[1] in sentence):\n",
    "                emotion_holder[emotion] +=1\n",
    "                \n",
    "    # emotion candidate with max value is the new label            \n",
    "    emotion_candid = max(emotion_holder, key=emotion_holder.get)\n",
    "    \n",
    "    # if sentence hasn't any frequent itemsets we don't label it\n",
    "    if(emotion_holder[emotion_candid] == 0 ):\n",
    "        return '-'\n",
    "    else:\n",
    "        return emotion_candid\n",
    "        \n",
    "    \n",
    "# new emotion label    \n",
    "attach_emotion_udf = udf(lambda x: attach_emotion(x), StringType())\n",
    "df = df.withColumn(\"new_emotion\", attach_emotion_udf(col(\"clean_text\")))\n",
    "\n",
    "print(\"Dataframe with new emotion label :\")\n",
    "display(df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving new emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab8cca0e-48c2-4906-bef0-6133555021d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# saving new_labels\n",
    "new_labels = df.select(col(\"id\"), col(\"new_emotion\")).toPandas()\n",
    "new_labels.to_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of our guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we select columns of given and new emotions, then just filter sentences that we could guess, then we compute number of true guesses and finally we compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78f7f6c0-8d7c-4d97-90fb-890d8d26cd55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our Guess is = 0.34462738067959725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# remove sentences that we couldn't guess\n",
    "guessed_rdd = df.select(col(\"emotions\"), col(\"new_emotion\")).rdd.filter(lambda row: row[1] != '-' )\n",
    "\n",
    "# number of true guesses\n",
    "true_guess = np.sum(guessed_rdd.map(lambda row: 1 if row[0] == row[1] else 0).collect())\n",
    "\n",
    "# number of all guesses\n",
    "guessed_number = guessed_rdd.count()\n",
    "\n",
    "print(\"Accuracy of our Guess is = \" + str(true_guess/guessed_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "2020-11-12 - DBFS Example",
   "notebookOrigID": 3943517766763034,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
